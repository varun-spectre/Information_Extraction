{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import os\n",
    "import pickle\n",
    "import plac\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import docx\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = [\"Agreement_start_date\"]\n",
    "# Loading training data \n",
    "with open ('./Train_Data_final', 'rb') as fp:\n",
    "    TRAIN_DATA = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# took from spacy docs\n",
    "LABEL = \"Agreement_start_date\"\n",
    "def ner_train(model=None, new_model_name=\"agreement_date\", output_dir=None, n_iter=30):\n",
    "    \"\"\"Set up the pipeline and entity recognizer, and train the new entity.\"\"\"\n",
    "    random.seed(0)\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "    # Add entity recognizer to model if it's not in the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    ner.add_label(LABEL)  # add new entity label to entity recognizer\n",
    "    # Adding extraneous labels shouldn't mess anything up\n",
    "    ner.add_label(\"VEGETABLE\")\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        optimizer = nlp.resume_training()\n",
    "    move_names = list(ner.move_names)\n",
    "    # get names of other pipes to disable them during training\n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    # only train NER\n",
    "    with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
    "        # show warnings for misaligned entity spans once\n",
    "        warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "\n",
    "        sizes = compounding(1.0, 4.0, 1.001)\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            batches = minibatch(TRAIN_DATA, size=sizes)\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "    # test the trained model\n",
    "    test_text = \"THIS RENTAL AGREEMENT is made and executed into on this first day of September 2011 (01-09-2011) at Bangalore\"\n",
    "    print(\"********************\")\n",
    "    doc = nlp(test_text)\n",
    "    print(\"Entities in '%s'\" % test_text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.label_, ent.text)\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta[\"name\"] = new_model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        # Check the classes have loaded back consistently\n",
    "        assert nlp2.get_pipe(\"ner\").move_names == move_names\n",
    "        doc2 = nlp2(test_text)\n",
    "        for ent in doc2.ents:\n",
    "            print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "Losses {'ner': 325.84619755458}\n",
      "Losses {'ner': 607.0687399369272}\n",
      "Losses {'ner': 388.9864876166245}\n",
      "Losses {'ner': 1095.3322046393891}\n",
      "Losses {'ner': 369.0838201758012}\n",
      "Losses {'ner': 118.78022780315874}\n",
      "Losses {'ner': 46.827406995610076}\n",
      "Losses {'ner': 22.119562175418736}\n",
      "Losses {'ner': 6.944464746641154}\n",
      "Losses {'ner': 8.539448460979234}\n",
      "Losses {'ner': 19.300927756450218}\n",
      "Losses {'ner': 13.12943070552477}\n",
      "Losses {'ner': 6.476039304835066}\n",
      "Losses {'ner': 10.572703672951816}\n",
      "Losses {'ner': 11.690214143992803}\n",
      "Losses {'ner': 6.022254871822959}\n",
      "Losses {'ner': 9.090501485605287}\n",
      "Losses {'ner': 5.752521776278737}\n",
      "Losses {'ner': 5.117576874731772}\n",
      "Losses {'ner': 6.0765005574275675}\n",
      "Losses {'ner': 15.494011484397273}\n",
      "Losses {'ner': 8.321595338182908}\n",
      "Losses {'ner': 2.76467962094256}\n",
      "Losses {'ner': 5.112262194129607}\n",
      "Losses {'ner': 7.852200318392593}\n",
      "Losses {'ner': 5.796727666946247}\n",
      "Losses {'ner': 5.495921599590493}\n",
      "Losses {'ner': 7.210881793490394}\n",
      "Losses {'ner': 7.61969650601068}\n",
      "Losses {'ner': 7.5374657179863425}\n",
      "********************\n",
      "Entities in 'THIS RENTAL AGREEMENT is made and executed into on this first day of September 2011 (01-09-2011) at Bangalore'\n",
      "Agreement_start_date first day of September 2011\n",
      "Saved model to output\n",
      "Loading from output\n",
      "Agreement_start_date first day of September 2011\n"
     ]
    }
   ],
   "source": [
    "ner_train(output_dir = \"./output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agreement_start_date --> 1st May 2005\n"
     ]
    }
   ],
   "source": [
    "# testing the model\n",
    "output_dir = \"./output\"\n",
    "test_text = \"\"\"This Rental Agreement is made and executed at Bangalore on this the 1st May 2005 by and between:\"\"\"\n",
    "nlp2 = spacy.load(output_dir)\n",
    "# Check the classes have loaded back consistently\n",
    "# assert nlp2.get_pipe(\"ner\").move_names == move_names\n",
    "doc2 = nlp2(test_text)\n",
    "for ent in doc2.ents:\n",
    "    print(ent.label_, \"-->\", ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agreement_start_date --> first day of September 2011\n",
      "Agreement_start_date --> 1st May 2005\n",
      "Agreement_start_date --> 15th of December 2012\n",
      "Agreement_start_date --> 2ndth day of July, 2013\n",
      "Agreement_start_date --> 06th day of March 2013\n",
      "Agreement_start_date --> 1st day of April 2008\n",
      "Agreement_start_date --> 11 (Eleven\n",
      "Agreement_start_date --> 07-072014\n",
      "Agreement_start_date --> 2/\n"
     ]
    }
   ],
   "source": [
    "# lets try to find the agreement start date in Validation folder\n",
    "# we got 5 out of 8 files to be right!!! we need more training data and some regex rules\n",
    "output_dir = \"./output\"\n",
    "nlp2 = spacy.load(output_dir)\n",
    "Data_Folder = \"./Validation_Data/\"\n",
    "def extract_entities(Data_Folder):\n",
    "    for filename in os.listdir(Data_Folder):\n",
    "        if filename.endswith(\".docx\"):\n",
    "            doc = docx.Document(os.path.join(Data_Folder, filename))\n",
    "            for para in doc.paragraphs:\n",
    "                doc2 = nlp2(para.text)\n",
    "                if len(doc2.ents) > 0:\n",
    "                    for ent in doc2.ents:\n",
    "                        print(ent.label_, \"-->\", ent.text)\n",
    "                    break\n",
    "extract_entities(Data_Folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
